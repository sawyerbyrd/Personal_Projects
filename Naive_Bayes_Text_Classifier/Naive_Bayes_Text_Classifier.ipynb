{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sawyer Byrd\n",
    "\n",
    "CMSC422 HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91281/1911068966.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#imports \n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import chardet\n",
    "import math\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting directory\n",
    "directory = Path('/home/sawyerbyrd/CMSC422/HW1/20_newsgroups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "puting label names into list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.os.ms-windows.misc', 'comp.graphics', 'sci.crypt', 'rec.sport.baseball', 'comp.windows.x', 'rec.motorcycles', 'rec.autos', 'soc.religion.christian', 'talk.politics.misc', 'talk.politics.guns', 'sci.electronics', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'alt.atheism', 'talk.religion.misc', 'sci.space', 'sci.med', 'rec.sport.hockey', 'misc.forsale', 'talk.politics.mideast']\n"
     ]
    }
   ],
   "source": [
    "# index of label in list will corespond to label number.\n",
    "# e.g. 'comp.graphics' is at index 1 so its label number will be 1.\n",
    "\n",
    "labels = []\n",
    "\n",
    "for file_path in directory.iterdir(): \n",
    "   labels.append(file_path.name)\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function that removes the first 4 lines of each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_first_4(file_path): \n",
    "    file = file_path.open('r', errors='ignore')\n",
    "    lines = file.readlines()\n",
    "    return lines[4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that tokenizes text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the cleaned, lowercase, tokenized text.\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    # collects only alphanumeric and spaces \n",
    "    cleaned = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    # sorts though for words of length >= 2\n",
    "    tokens = re.findall(r'\\b[a-z]{2,}\\b', cleaned)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entering docs into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each row has: col1 -> doc contents ; col2 -> label\n",
    "\n",
    "# list that will be used to create dataframe\n",
    "docs = []\n",
    "\n",
    "# iterating through class files\n",
    "for label_path in directory.iterdir():\n",
    "    # iterating through each doc \n",
    "    for file_path in label_path.iterdir():\n",
    "        if file_path.is_file():\n",
    "            # removing the first 4 lines, making it into one string and tokenizing it\n",
    "            doc_content = remove_first_4(file_path)\n",
    "            doc_content = tokenize(''.join(doc_content))\n",
    "            docs.append({\n",
    "                'Contents': doc_content,\n",
    "                'Label': labels.index(label_path.name)\n",
    "            })\n",
    "\n",
    "all_docs = pd.DataFrame(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contents</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[subject, lockups, in, enh, mode, floppy, mess...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[message, id, qgsb, world, std, com, followup,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[date, apr, organization, center, for, reliabl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[keywords, winprinter, from, lasermaster, corp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[message, id, silver, sfu, ca, sender, news, s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>[subject, desertification, of, the, negev, mes...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>[subject, re, israeli, terrorism, date, apr, g...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>[subject, the, soviet, armenian, government, m...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>[subject, arrest, of, fugitive, in, adl, case,...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>[date, may, gmt, organization, university, of,...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Contents  Label\n",
       "0     [subject, lockups, in, enh, mode, floppy, mess...      0\n",
       "1     [message, id, qgsb, world, std, com, followup,...      0\n",
       "2     [date, apr, organization, center, for, reliabl...      0\n",
       "3     [keywords, winprinter, from, lasermaster, corp...      0\n",
       "4     [message, id, silver, sfu, ca, sender, news, s...      0\n",
       "...                                                 ...    ...\n",
       "9995  [subject, desertification, of, the, negev, mes...     19\n",
       "9996  [subject, re, israeli, terrorism, date, apr, g...     19\n",
       "9997  [subject, the, soviet, armenian, government, m...     19\n",
       "9998  [subject, arrest, of, fugitive, in, adl, case,...     19\n",
       "9999  [date, may, gmt, organization, university, of,...     19\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_temp = []\n",
    "test_temp = []\n",
    "# half of the docs from each class in train and half in test\n",
    "for label in range(20): \n",
    "    class_df = all_docs[all_docs['Label'] == label]\n",
    "    # shuffling the class set before spliting\n",
    "    class_df = class_df.sample(frac=1, random_state=37).reset_index(drop=True)\n",
    "    train_temp.append(class_df[:500])\n",
    "    test_temp.append(class_df[500:])\n",
    "\n",
    "# concat all dfs for train and test into one df each\n",
    "# shuffling test for randomness\n",
    "train = pd.concat(train_temp).reset_index(drop=True)\n",
    "test = pd.concat(test_temp).reset_index(drop=True)\n",
    "test = test.sample(frac=1, random_state=37).reset_index(drop=True)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organizing some data for training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each index in this array holds a dictionary for the counts of unique words in the cooresponding class\n",
    "# ie. index 0 holds the counts for all unique words in class 0 ('comp.os.ms-windows.misc')\n",
    "class_wrd_ct = [Counter() for _ in range(20)]\n",
    "\n",
    "# this holds the count of each unique word in the entirety of the train set\n",
    "# ie. the vocab set\n",
    "vocab = Counter()\n",
    "\n",
    "# for each row, update the class at index (label) and vocab with word count for that docs content\n",
    "for i, row in train.iterrows():\n",
    "    cont = row['Contents']\n",
    "    label = row['Label']\n",
    "    class_wrd_ct[label].update(cont)\n",
    "    vocab.update(cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a stop list and removing those words from V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_lst = set({word for word, _ in vocab.most_common(200)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop list from vocab\n",
    "vocab = set({\n",
    "    word: word\n",
    "    for word in vocab.keys()\n",
    "    if word not in stop_lst\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up constant for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary:  83330\n"
     ]
    }
   ],
   "source": [
    "voc_len = len(vocab)\n",
    "print('Length of Vocabulary: ', voc_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and populating loglikelyhood dictionary\n",
    "\n",
    "log_likelyhood = {}\n",
    "\n",
    "for cls in range(20):\n",
    "    # for each class, create a log likeleyhood entry \n",
    "    log_likelyhood[cls] = {}\n",
    "    # for each word in vocab\n",
    "    for word in vocab:\n",
    "        count_w_c = class_wrd_ct[cls].get(word, 0) + 1\n",
    "        class_ct = len(class_wrd_ct[cls]) + voc_len\n",
    "        # log likeleyhood[w,c] = (count(w,c) + 1) / (how many times it shows up in cls + |V|)\n",
    "        log_likelyhood[cls][word] = math.log(count_w_c / class_ct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up constant for tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logprior:  -2.995732273553991\n"
     ]
    }
   ],
   "source": [
    "logprior = math.log(1000/20000)\n",
    "print('Logprior: ', logprior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the test algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive(test_doc):\n",
    "    sum = [logprior] * 20\n",
    "    for c in range(20):\n",
    "        for word in test_doc:\n",
    "            if word in vocab:\n",
    "                sum[c] = sum[c] + log_likelyhood[c][word]\n",
    "    \n",
    "    arg_max_c = max(sum)\n",
    "    cls = sum.index(arg_max_c)\n",
    "    return cls\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new column in the dataframe to represent the classification (i.e. y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['y_hat'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating variables to keep track of correct classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable to count the number of correct classifications\n",
    "correct = 0\n",
    "\n",
    "# dictionary to count the number of correct classifications in each class\n",
    "class_correct = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runing each test doc through algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in test.iterrows():\n",
    "    # updating y_hat to the algorithms guess for what class this doc is in\n",
    "    y_hat = test_naive(row['Contents'])\n",
    "    row['y_hat'] = y_hat\n",
    "    # updating the number of correct classifications\n",
    "    label = row['Label']\n",
    "    if label == y_hat:\n",
    "        correct += 1\n",
    "        if label not in class_correct:\n",
    "            class_correct[label] = 0\n",
    "        class_correct[label] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the total accuracy, as well as for each individual class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Correct:  8187\n",
      "Percentage Correct:  81.89456837051115 %\n",
      "\n",
      "Class:  sci.med\n",
      "percent_correct:  89.4 %\n",
      "\n",
      "Class:  sci.crypt\n",
      "percent_correct:  94.39999999999999 %\n",
      "\n",
      "Class:  rec.motorcycles\n",
      "percent_correct:  91.8 %\n",
      "\n",
      "Class:  sci.space\n",
      "percent_correct:  93.0 %\n",
      "\n",
      "Class:  soc.religion.christian\n",
      "percent_correct:  98.0 %\n",
      "\n",
      "Class:  talk.politics.misc\n",
      "percent_correct:  79.60000000000001 %\n",
      "\n",
      "Class:  comp.graphics\n",
      "percent_correct:  80.60000000000001 %\n",
      "\n",
      "Class:  sci.electronics\n",
      "percent_correct:  72.0 %\n",
      "\n",
      "Class:  comp.sys.mac.hardware\n",
      "percent_correct:  70.6 %\n",
      "\n",
      "Class:  talk.politics.mideast\n",
      "percent_correct:  96.8 %\n",
      "\n",
      "Class:  talk.religion.misc\n",
      "percent_correct:  58.599999999999994 %\n",
      "\n",
      "Class:  talk.politics.guns\n",
      "percent_correct:  89.4 %\n",
      "\n",
      "Class:  rec.sport.baseball\n",
      "percent_correct:  88.6 %\n",
      "\n",
      "Class:  comp.windows.x\n",
      "percent_correct:  89.0 %\n",
      "\n",
      "Class:  rec.sport.hockey\n",
      "percent_correct:  96.6 %\n",
      "\n",
      "Class:  misc.forsale\n",
      "percent_correct:  48.8 %\n",
      "\n",
      "Class:  comp.sys.ibm.pc.hardware\n",
      "percent_correct:  74.8 %\n",
      "\n",
      "Class:  alt.atheism\n",
      "percent_correct:  76.2 %\n",
      "\n",
      "Class:  comp.os.ms-windows.misc\n",
      "percent_correct:  65.60000000000001 %\n",
      "\n",
      "Class:  rec.autos\n",
      "percent_correct:  83.6 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total accuracy\n",
    "print('Number Correct: ', correct)\n",
    "percent_correct = (correct/len(test)) * 100\n",
    "\n",
    "# Class accuracy\n",
    "print('Percentage Correct: ', percent_correct, '%\\n')\n",
    "for key, value in class_correct.items():\n",
    "    print('Class: ', labels[key])\n",
    "    print('percent_correct: ', (value/500) * 100, '%\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
